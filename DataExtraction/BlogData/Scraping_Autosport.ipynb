{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d90ec84-78df-4001-91c5-7deecb7cbbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d099ea-0e76-42e5-b89a-605ecec96a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUrls():\n",
    "    base_url, blog_urls = \"https://www.autosport.com\", []\n",
    "    for page_nr in range(22):\n",
    "        page = requests.get(base_url + \"/live/?p=\" + str(page_nr))\n",
    "        soup = bs(page.content, \"html.parser\")\n",
    "        blocks = soup.find_all(\"a\", class_=\"ms-item--live-text\")\n",
    "        for block in blocks:\n",
    "            title = block.find_all(\"p\", class_=\"ms-item__title\")[0].text\n",
    "            if \"race\" in title.lower():\n",
    "                blog_urls.append(base_url + block['href'])\n",
    "    return [u for u in blog_urls if 'f1' in u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d27d12a2-1c11-4355-b2d5-d83c00e2b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBlogs(url):\n",
    "    base_url = \"https://www.autosport.com\"\n",
    "    try: page = requests.get(url)\n",
    "    except: return None\n",
    "    soup = bs(page.content, \"html.parser\")\n",
    "    status = soup.find_all(\"span\", class_=\"mslt_status-value\")[0].text\n",
    "    if status != \"Stopped\": return None\n",
    "    first_post = soup.find(\"div\", class_=\"mslt-msg_text\")\n",
    "    url_title = url.split('-text/')[1]\n",
    "    if 'gp' in url_title: url_title = url_title.split('-gp')[0]\n",
    "    elif 'grand' in url_title: url_title = url_title.split('-grand')[0]\n",
    "    if 'f1-' in url_title: url_title = url_title.replace('f1-', '')\n",
    "    url_date = first_post.find_all(\"time\", class_=\"mslt-msg_time\")[0]['datetime'][:4]\n",
    "    button = soup.find(\"a\", attrs={\"data-id\": \"nextPage\"})\n",
    "    if not button: return None\n",
    "    but_href = button['href'][:-1]\n",
    "    empty_msg, page, items = False, 1, {}\n",
    "    while not empty_msg:\n",
    "        data_url = base_url + but_href + str(page)\n",
    "        resp = requests.get(data_url)\n",
    "        json_data = json.loads(resp.content)\n",
    "        if json_data['html'] == \"\": empty_msg = True\n",
    "        else:\n",
    "            html = json_data['html']\n",
    "            soup_nw = bs(html, \"html.parser\")\n",
    "            posts = soup_nw.find_all(\"div\", class_=\"mslt-msg_text\")\n",
    "            for post in posts:\n",
    "                datetime = post.find_all(\"time\", class_=\"mslt-msg_time\")[0]['datetime']\n",
    "                content = post.find_all(\"span\", class_=\"mslt-msg_text-content\")[0].text\n",
    "                if '\"' in content: continue\n",
    "                items[datetime] = content\n",
    "            page += 1\n",
    "    filename = \"DataSet/\" + url_date + \"_\" + url_title + \".json\"\n",
    "    with open(filename, 'w') as f: json.dump(items, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d1a09a4-3ec5-445c-81f8-705a05dc311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_urls = getUrls()\n",
    "\n",
    "for url in blog_urls:\n",
    "    getBlogs(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (expenv)",
   "language": "python",
   "name": "expenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
